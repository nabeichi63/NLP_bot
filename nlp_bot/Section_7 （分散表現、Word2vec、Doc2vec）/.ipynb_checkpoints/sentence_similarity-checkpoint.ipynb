{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章の類似度\n",
    "Doc2Vecは、任意の長さの文書をベクトル化する技術です。  \n",
    "この技術を使って、文書やテキストの分散表現を獲得し、類似度を計算します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み\n",
    "以前に作成したデータの読み込みを行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['吾輩', 'は', '猫', 'で', 'ある', '。'], ['名前', 'は', 'まだ', '無い', '。'], ['どこ', 'で', '生れ', 'た', 'か', 'とんと', '見当', 'が', 'つか', 'ぬ', '。'], ['何', 'でも', '薄暗い', 'じめじめ', 'し', 'た', '所', 'で', 'ニャーニャー', '泣い', 'て', 'いた事', 'だけ', 'は', '記憶', 'し', 'て', 'いる', '。'], ['吾輩', 'は', 'ここ', 'で', '始め', 'て', '人間', 'という', 'もの', 'を', '見', 'た', '。'], ['しかも', 'あと', 'で', '聞く', 'と', 'それ', 'は', '書生', 'という', '人間', '中', 'で', '一番', '獰悪', 'な', '種族', 'で', 'あっ', 'た', 'そう', 'だ', '。'], ['この', '書生', 'という', 'の', 'は', '時々', '我々', 'を', '捕え', 'て', '煮', 'て', '食う', 'という', '話', 'で', 'ある', '。'], ['しかし', 'その', '当時', 'は', '何', 'という', '考', 'も', 'なかっ', 'た', 'から', '別段', '恐し', 'いとも', '思わ', 'なかっ', 'た', '。'], ['ただ', '彼', 'の', '掌', 'に', '載せ', 'られ', 'て', 'スー', 'と', '持ち上げ', 'られ', 'た', '時', '何だか', 'フワフワ', 'し', 'た', '感じ', 'が', 'あっ', 'た', 'ばかり', 'で', 'ある', '。'], ['掌', 'の', '上', 'で', '少し', '落ちつい', 'て', '書生', 'の', '顔', 'を', '見', 'た', 'の', 'が', 'いわゆる', '人間', 'という', 'もの', 'の', '見', '始', 'で', 'あろ', 'う', '。']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('wagahai_words.pickle', mode='rb') as f:\n",
    "    wagahai_words = pickle.load(f)\n",
    "\n",
    "print(wagahai_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vecによる学習\n",
    "doc2vecを使って学習を行い、モデルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['吾輩', 'は', '猫', 'で', 'ある', '。'], tags=[0]), TaggedDocument(words=['名前', 'は', 'まだ', '無い', '。'], tags=[1]), TaggedDocument(words=['どこ', 'で', '生れ', 'た', 'か', 'とんと', '見当', 'が', 'つか', 'ぬ', '。'], tags=[2]), TaggedDocument(words=['何', 'でも', '薄暗い', 'じめじめ', 'し', 'た', '所', 'で', 'ニャーニャー', '泣い', 'て', 'いた事', 'だけ', 'は', '記憶', 'し', 'て', 'いる', '。'], tags=[3]), TaggedDocument(words=['吾輩', 'は', 'ここ', 'で', '始め', 'て', '人間', 'という', 'もの', 'を', '見', 'た', '。'], tags=[4]), TaggedDocument(words=['しかも', 'あと', 'で', '聞く', 'と', 'それ', 'は', '書生', 'という', '人間', '中', 'で', '一番', '獰悪', 'な', '種族', 'で', 'あっ', 'た', 'そう', 'だ', '。'], tags=[5]), TaggedDocument(words=['この', '書生', 'という', 'の', 'は', '時々', '我々', 'を', '捕え', 'て', '煮', 'て', '食う', 'という', '話', 'で', 'ある', '。'], tags=[6]), TaggedDocument(words=['しかし', 'その', '当時', 'は', '何', 'という', '考', 'も', 'なかっ', 'た', 'から', '別段', '恐し', 'いとも', '思わ', 'なかっ', 'た', '。'], tags=[7]), TaggedDocument(words=['ただ', '彼', 'の', '掌', 'に', '載せ', 'られ', 'て', 'スー', 'と', '持ち上げ', 'られ', 'た', '時', '何だか', 'フワフワ', 'し', 'た', '感じ', 'が', 'あっ', 'た', 'ばかり', 'で', 'ある', '。'], tags=[8]), TaggedDocument(words=['掌', 'の', '上', 'で', '少し', '落ちつい', 'て', '書生', 'の', '顔', 'を', '見', 'た', 'の', 'が', 'いわゆる', '人間', 'という', 'もの', 'の', '見', '始', 'で', 'あろ', 'う', '。'], tags=[9])]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "tagged_documents = []\n",
    "for i, sentence in enumerate(wagahai_words):\n",
    "    tagged_documents.append(TaggedDocument(sentence, [i]))  # TaggedDocument型のオブジェクトをリストに格納\n",
    "    \n",
    "print(tagged_documents[:10])\n",
    "\n",
    "# size：分散表現の次元数（中間層のニューロン数）\n",
    "# window：対象単語を中心とした前後の単語数\n",
    "# min_count：学習に使う単語の最低出現回数\n",
    "# epochs:epochs数\n",
    "# dm：学習モデル=DBOW（デフォルトはdm=1で、学習モデルはDM）\n",
    "model = Doc2Vec(documents=tagged_documents,\n",
    "                vector_size=100,\n",
    "                min_count=5,\n",
    "                window=5,\n",
    "                epochs=20,\n",
    "                dm=0)           # この場合は、DBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章のベクトル\n",
    "最初の文章のベクトルを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吾輩', 'は', '猫', 'で', 'ある', '。']\n",
      "[ 0.03472404  0.04284214 -0.05396239  0.01737194  0.03893199 -0.11020339\n",
      "  0.06253986 -0.13551162 -0.01626237  0.04526579  0.05106853  0.02652687\n",
      "  0.01685416 -0.07984128 -0.23986095  0.08646646 -0.06009866  0.02403493\n",
      " -0.10466882 -0.06882231 -0.075416   -0.10884568  0.03905427  0.01291232\n",
      " -0.0964826   0.11402889  0.01016321  0.06658836 -0.05154552  0.01469842\n",
      " -0.04320029  0.00448598 -0.1061907  -0.09498504 -0.00159009  0.14791337\n",
      " -0.04531574  0.05143563  0.02204468  0.09817099 -0.07764384 -0.10305659\n",
      "  0.04200853  0.2259245  -0.03185333  0.03828705  0.09054005  0.10499027\n",
      "  0.0341304   0.05859769 -0.10089565 -0.12037896 -0.00251839  0.02388112\n",
      "  0.11085957 -0.02640626  0.0071974   0.05353459 -0.12946017 -0.06867552\n",
      " -0.09582026 -0.04360649  0.10285255 -0.06208378 -0.02539206  0.07335379\n",
      "  0.14483501 -0.1632261   0.01422753  0.00365141  0.03061519 -0.02170982\n",
      "  0.0950296  -0.06545531  0.01719221 -0.13624285 -0.07315962  0.08122993\n",
      " -0.04948763  0.01365321  0.03124141  0.15104102  0.00665838  0.09780336\n",
      " -0.03379446 -0.16469525  0.06400836  0.0369223  -0.05865261 -0.00934053\n",
      " -0.04044765 -0.10884951 -0.05279087  0.05922592  0.10432144  0.02648882\n",
      " -0.04354286 -0.00311453 -0.02523632 -0.06849568]\n"
     ]
    }
   ],
   "source": [
    "print(wagahai_words[0])  # 最初の文章を表示\n",
    "print(model.docvecs[0])  # 最初の文章のベクトル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章の類似度\n",
    "最も類似度の高い文章のIDと類似度を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5605, 0.9988423585891724), (79, 0.9984862208366394), (3782, 0.9982940554618835), (425, 0.998279869556427), (2818, 0.9980063438415527), (210, 0.9979227781295776), (5885, 0.9978996515274048), (530, 0.9978892803192139), (3695, 0.9978214502334595), (157, 0.9978139400482178)]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs.most_similar(0))    # id が 0 の文章、すなわち、最初の文章と類似度の高い文章を出力。出力は id と 類似度の組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吾輩', 'は', '猫', 'で', 'ある', '。']\n",
      "['吾輩', 'は', '猫', 'ながら', '時々', '考える', '事', 'が', 'ある', '。']\n",
      "['吾輩', 'は', '淡泊', 'を', '愛する', '茶人', '的', '猫', 'で', 'ある', '。']\n",
      "['吾輩', 'も', 'この', '頃', 'で', 'は', '普通', '一般', 'の', '猫', 'で', 'は', 'ない', '。']\n",
      "['吾輩', 'は', '日本', 'の', '猫', 'だ', 'から', '無論', '日本', '贔負', 'で', 'ある', '。']\n",
      "['車屋', 'の', '黒', 'は', 'この', '近辺', 'で', '知ら', 'ぬ', '者', 'なき', '乱暴', '猫', 'で', 'ある', '。']\n",
      "['吾輩', 'も', '日本', 'の', '猫', 'だ', 'から', '多少', 'の', '愛国心', 'は', 'ある', '。']\n",
      "['吾輩', 'は', '猫', 'で', 'は', 'ある', 'が', '大抵', 'の', 'もの', 'は', '食う', '。']\n",
      "['猫', 'の', 'うち', 'で', 'この', '芸', 'が', '出来る', '者', 'は', '恐らく', '吾輩', 'のみ', 'で', 'あろ', 'う', '。']\n",
      "['吾輩', 'は', '猫', 'として', '決して', '上乗', 'の', '出来', 'で', 'は', 'ない', '。']\n"
     ]
    }
   ],
   "source": [
    "for p in model.docvecs.most_similar(0):\n",
    "    print(wagahai_words[p[0]])      # p[0]は、id を示す "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "類似度の高い文章が表示されました。  \n",
    "doc2vecにより、文書全体の類似度を計算することも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題:\n",
    "wagahai_wordsの中の適当な文章と、類似度の高い文章を表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target の文章:  名前はまだ無い。\n",
      "似ている文章：\n",
      "1: ちっとも知らなかった。\n",
      "2: 何だか分らんと主人が答える。\n",
      "3: それでもまだやめない。\n",
      "4: 馬鹿野郎。\n",
      "5: どうも鼠らしくない。\n",
      "6: ええありがとう。\n",
      "7: 早々棄てなさい。\n",
      "8: しかし話しは過去へ溯らんと源因が分からない。\n",
      "9: それから頬っぺたにかかる。\n",
      "10: タカジヤスターゼは無論いかん。\n"
     ]
    }
   ],
   "source": [
    "s_id = 1\n",
    "print('Target の文章: ',  ''.join(wagahai_words[s_id]))\n",
    "print('似ている文章：')\n",
    "for no, (sid, similarity) in enumerate(model.docvecs.most_similar(s_id), 1):\n",
    "    print(f\"{no}: {''.join(wagahai_words[sid])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
